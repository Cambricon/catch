diff --git a/torch/csrc/distributed/c10d/reducer.cpp b/torch/csrc/distributed/c10d/reducer.cpp
index 097525a..46e3a7a 100644
--- a/torch/csrc/distributed/c10d/reducer.cpp
+++ b/torch/csrc/distributed/c10d/reducer.cpp
@@ -137,12 +137,20 @@ Reducer::Reducer(
 
     for (size_t i = 0; i < replica_count; i++) {
       at::TensorOptions options;
-      options = options.dtype(at::kInt);
+      if (replicas_[i][0].device().type() == c10::DeviceType::MLU) {
+        options = options.dtype(at::kFloat);
+      } else {
+        options = options.dtype(at::kInt);
+      }
 
       if (replicas_[i][0].is_cuda()) {
         at::DeviceGuard g(replicas_[i][0].device());
         local_used_maps_[i] = at::zeros(
             {static_cast<long>(variable_count)}, options.pinned_memory(true));
+      } else if (replicas_[i][0].device().type() == c10::DeviceType::MLU) {
+        auto cpu_pinned_tensor = at::empty_pinned(
+            {static_cast<long>(variable_count)}, options.device(c10::DeviceType::MLU));
+        local_used_maps_[i] = cpu_pinned_tensor.zero_();
       } else {
         local_used_maps_[i] =
             at::zeros({static_cast<long>(variable_count)}, options);
@@ -316,7 +324,11 @@ void Reducer::mark_variable_ready_dense(VariableIndex index) {
       auto wrapped = c10::scalar_to_tensor(double(1.)/process_group_->getSize());
       wrapped.unsafeGetTensorImpl()->set_wrapped_number(true);
       // Divides while copying into the bucket view.
-      at::native::mul_out(bucket_view, grad, wrapped);
+      if (grad.device().type() == c10::DeviceType::MLU) {
+        at::mul_out(bucket_view, grad, wrapped);
+      } else {
+        at::native::mul_out(bucket_view, grad, wrapped);
+      }
     } else {
       bucket_view.zero_();
     }
@@ -493,18 +505,33 @@ void Reducer::mark_variable_ready(VariableIndex index) {
         guard.getStream(replica.contents.device());
     torch::autograd::Engine::get_default_engine().queue_callback([=] {
       std::unique_lock<std::mutex> lock(this->mutex_);
-      // Run callback with the current stream
-      c10::OptionalStreamGuard currentStreamGuard{currentStream};
-      this->finalize_backward();
-      // Rebuild bucket if this is the first time to rebuild
-      if (!rebuilt_params_.empty()) {
-        auto rebuilt_bucket_indices = rebuildBuckets();
-        // Unlock before initialize_buckets() as initialize_buckets() requires a
-        // lock, it could result in self deadlock without unlocking here.
-        lock.unlock();
-        initialize_buckets(std::move(rebuilt_bucket_indices));
+      // MLU currently do not support this stream guard usage
+      if (replica.contents.device().type() != c10::DeviceType::MLU) {
+        // Run callback with the current stream
+        c10::OptionalStreamGuard currentStreamGuard{currentStream};
+        this->finalize_backward();
+        // Rebuild bucket if this is the first time to rebuild
+        if (!rebuilt_params_.empty()) {
+          auto rebuilt_bucket_indices = rebuildBuckets();
+          // Unlock before initialize_buckets() as initialize_buckets() requires a
+          // lock, it could result in self deadlock without unlocking here.
+          lock.unlock();
+          initialize_buckets(std::move(rebuilt_bucket_indices));
+        } else {
+          lock.unlock();
+        }
       } else {
-        lock.unlock();
+        this->finalize_backward();
+        // Rebuild bucket if this is the first time to rebuild
+        if (!rebuilt_params_.empty()) {
+          auto rebuilt_bucket_indices = rebuildBuckets();
+          // Unlock before initialize_buckets() as initialize_buckets() requires a
+          // lock, it could result in self deadlock without unlocking here.
+          lock.unlock();
+          initialize_buckets(std::move(rebuilt_bucket_indices));
+        } else {
+          lock.unlock();
+        }
       }
     });
   }
@@ -672,8 +699,15 @@ void Reducer::initialize_buckets(
           if (v.is_non_overlapping_and_dense()) {
             // If the param's memory is dense, match its layout, anticipating the autograd engine
             // (AccumulateGrad) will also create gradients matching its layout.
-            replica.bucket_views.push_back(replica.contents
-                                           .as_strided(v.sizes(), v.strides(), offset));
+            if (v.device().type() == c10::DeviceType::MLU) {
+              // MLU as_strided will create a new tensor, can't be used here.
+              auto bucket_view = at::empty(v.sizes(), options);
+              replica.bucket_views.push_back(bucket_view.set_(replica.contents.storage(),
+                                             offset, v.sizes(), v.strides()));
+            } else {
+              replica.bucket_views.push_back(replica.contents
+                                             .as_strided(v.sizes(), v.strides(), offset));
+            }
           } else {
             // Fall back to a C-style contiguous view, again anticipating AccumulateGrad will do
             // the same when stashing grads for non-dense params.
diff --git a/torch/cuda/_utils.py b/torch/cuda/_utils.py
index 8d3dc59..3b590e8 100644
--- a/torch/cuda/_utils.py
+++ b/torch/cuda/_utils.py
@@ -22,17 +22,21 @@ def _get_device_index(device: Union[Device, int], optional=False) -> int:
     device_idx: Optional[int]
     if isinstance(device, torch.device):
         dev_type = device.type
-        if device.type != 'cuda':
-            raise ValueError('Expected a cuda device, but got: {}'.format(device))
+        if device.type != 'cuda' and device.type != 'mlu':
+            raise ValueError('Expected a cuda or mlu device, but got: {}'.format(device))
         device_idx = device.index
     else:
         device_idx = device
     if device_idx is None:
         if optional:
             # default cuda device index
-            return torch.cuda.current_device()
+            if torch.is_mlu_available():
+                import torch_mlu.core.mlu_model as ct
+                return ct.current_device()
+            else:
+                return torch.cuda.current_device()
         else:
-            raise ValueError('Expected a cuda device with a specified index '
+            raise ValueError('Expected a cuda or mlu device with a specified index '
                              'or an integer, but got: {}'.format(device))
     return device_idx
 
diff --git a/torch/nn/parallel/distributed.py b/torch/nn/parallel/distributed.py
index 5b4c6b6..5abb7c2 100644
--- a/torch/nn/parallel/distributed.py
+++ b/torch/nn/parallel/distributed.py
@@ -273,8 +273,11 @@ class DistributedDataParallel(Module):
 
         self.is_multi_device_module = len({p.device for p in module.parameters()}) > 1
         self.is_cuda = all([p.device.type == 'cuda' for p in module.parameters()])
+        if torch.is_mlu_available():
+            import torch_mlu.core.mlu_model as ct
+            self.is_mlu = all([p.device.type == 'mlu' for p in module.parameters()])
 
-        if not self.is_cuda or self.is_multi_device_module:
+        if not (self.is_cuda or (hasattr(self, 'is_mlu') and self.is_mlu)) or self.is_multi_device_module:
             assert not device_ids and not output_device, (
                 "DistributedDataParallel device_ids and output_device arguments "
                 "only work with single-device CUDA modules, but got "
@@ -286,7 +289,14 @@ class DistributedDataParallel(Module):
         else:
             # Use all devices by default for single-device CUDA modules
             if device_ids is None:
-                device_ids = list(range(torch.cuda.device_count()))
+                assert not hasattr(self, 'is_mlu') or ct.device_count() == 1, (
+                    "MLU DistributedDataParallel currently only support one device per process mode, "
+                    "device_ids arguments can not be None and there are multiple visible MLU devices."
+                )
+                if hasattr(self, 'is_mlu'):
+                    device_ids = [0]
+                else:
+                    device_ids = list(range(torch.cuda.device_count()))
 
             self.device_ids = list(map(lambda x: _get_device_index(x, True), device_ids))
 
@@ -295,6 +305,11 @@ class DistributedDataParallel(Module):
 
             self.output_device = _get_device_index(output_device, True)
 
+            assert not hasattr(self, 'is_mlu') or len(self.device_ids) == 1 and self.device_ids[0] == self.output_device, (
+                "MLU DistributedDataParallel currently only support one device per process mode, "
+                "bug got device_ids {}, output_device {}."
+            ).format(self.device_ids, self.output_device)
+
         if self.is_multi_device_module:
             assert self.is_cuda, (
                 "DistributedDataParallel with multi-device module only works "
@@ -394,8 +409,14 @@ class DistributedDataParallel(Module):
             self._module_copies = [self.module]

         self.modules_params = [list(parameters(m)) for m in self._module_copies]
+        assert not hasattr(self, 'is_mlu') or not self.is_mlu or len(self.modules_params[0]) == 0 or \
+        _get_device_index(self.modules_params[0][0].device, True) == self.device_ids[0], (
+            "MLU DistributedDataParallel currently only support one device per process mode, "
+            "but module parameters reside on a different device other than device_ids indicated."
+        )
         self.modules_buffers = [list(m.buffers()) for m in self._module_copies]

+
         # Build tuple of (module, parameter) for all parameters that require grads.
         modules_and_parameters = [
             [
@@ -505,7 +543,7 @@ class DistributedDataParallel(Module):
         if self.require_forward_param_sync:
             self._sync_params()
 
-        if self.device_ids:
+        if self.device_ids and not (hasattr(self, 'is_mlu') and self.is_mlu):
             inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)
             if len(self.device_ids) == 1:
                 output = self.module(*inputs[0], **kwargs[0])

diff --git a/torch/distributed/distributed_c10d.py b/torch/distributed/distributed_c10d.py
index bc1fab3..4e41d14 100644
--- a/torch/distributed/distributed_c10d.py
+++ b/torch/distributed/distributed_c10d.py
@@ -1,3 +1,4 @@
+import contextlib
 import torch
 import warnings
 from torch._six import string_classes
@@ -242,7 +243,29 @@ def _check_tensor_list(param, param_name):
        not all(isinstance(p, torch.Tensor) for p in param):
         raise RuntimeError("Invalid function argument. Expected parameter `{}` "
                            "to be of type List[torch.Tensor].".format(param_name))
+def _check_op(op):
+    """
+    Helper to check that the ``op`` is either isend or irecv.
+    """
+    if op not in [isend, irecv]:
+        raise RuntimeError("Invalid ``op``. Expected ``op`` "
+                           "to be of type ``torch.distributed.isend`` or "
+                           "``torch.distributed.irecv``.")
+
+def _check_p2p_op_list(p2p_op_list):
+    """
+    Helper to check that the ``p2p_op_list`` is a list of P2POp instances and
+    all ops use the same backend.
+    """
+    if not isinstance(p2p_op_list, list) or \
+       not all(isinstance(p2p_op, P2POp) for p2p_op in p2p_op_list):
+        raise RuntimeError("Invalid ``p2p_op_list``. Each op is expected to "
+                           "to be of type ``torch.distributed.P2POp``.")
+
 
+    backend = get_backend(p2p_op_list[0].group)
+    if not all(backend == get_backend(p2p_op.group) for p2p_op in p2p_op_list):
+        raise RuntimeError("All groups need to use the same backend.")
 
 def is_mpi_available():
     """
@@ -751,6 +774,94 @@ def recv(tensor,
             pg.recv([tensor], group_src_rank, tag).wait()
         return src
 
+class P2POp(object):
+    """
+    A class to build point-to-point operations for ``batch_isend_irecv``.
+
+    This class builds the type of P2P operation, communication buffer, peer rank,
+    Process Group group, and tag. Instances of this class will be passed to
+    ``batch_isend_irecv`` for point-to-point communications.
+
+    Args:
+        op (callable): A function to send data to or receive data from a peer process.
+            The type of ``op`` is either ``torch.distributed.isend`` or
+            ``torch.distributed.irecv``.
+        tensor (Tensor): Tensor to send or receive.
+        peer (int): Destination or source rank.
+        group (ProcessGroup, optional): The process group to work on. If None,
+            the default process group will be used.
+        tag (int, optional): Tag to match send with recv.
+    """
+    def __init__(self, op, tensor, peer, group=GroupMember.WORLD, tag=0):
+        self.op = op
+        self.tensor = tensor
+        self.peer = peer
+        self.group = group
+        self.tag = tag
+
+    def __new__(cls, op, tensor, peer, group=None, tag=0):
+        _check_op(op)
+        _check_single_tensor(tensor, "tensor")
+        return object.__new__(cls)
+
+@contextlib.contextmanager
+def _batch_p2p_manager(backend):
+    if backend == Backend.NCCL:
+        ProcessGroupNCCL._group_start()
+    try:
+        yield
+    finally:
+        if backend == Backend.NCCL:
+            ProcessGroupNCCL._group_end()
+
+def batch_isend_irecv(p2p_op_list):
+    """
+    Send or Receive a batch of tensors asynchronously and return a list of requests.
+
+    Process each of the operations in p2p_op_list and return the corresponding
+    requests. NCCL and Gloo backend are currently supported.
+
+    Args:
+        p2p_op_list: A list of point-to-point operations(type of each operator is
+            ``torch.distributed.P2POp``). The order of the isend/irecv in the list
+            matters and it needs to match with corresponding isend/irecv on the
+            remote end.
+
+    Returns:
+        A list of distributed request objects returned by calling the corresponding
+        op in the op_list.
+
+    Examples:
+        >>> send_tensor = torch.arange(2) + 2 * rank
+        >>> recv_tensor = torch.randn(2)
+        >>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)
+        >>> recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank + 1)%world_size)
+        >>> reqs = batch_isend_irecv([send_op, recv_op])
+        >>> for req in reqs:
+        >>>     req.wait()
+        >>> recv_tensor
+        tensor([2, 3])     # Rank 0
+        tensor([0, 1])     # Rank 1
+
+    .. note:: Note that when this API is used with the NCCL PG backend, users must set
+        the current GPU device with `torch.cuda.set_device`, otherwise it will
+        lead to unexpected hang issues.
+    """
+    _check_p2p_op_list(p2p_op_list)
+    backend = get_backend(p2p_op_list[0].group)
+    reqs = []
+    with _batch_p2p_manager(backend):
+        for p2p_op in p2p_op_list:
+            op = p2p_op.op
+            tensor = p2p_op.tensor
+            peer = p2p_op.peer
+            curr_group = p2p_op.group
+            tag = p2p_op.tag
+            ret = op(tensor, peer, curr_group, tag)
+
+            if ret is not None:
+                reqs.append(ret)
+    return reqs
 
 def broadcast_multigpu(tensor_list,
                        src,
